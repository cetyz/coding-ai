{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "from assistant.tools import fetch_github_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GPT_MODEL = 'gpt-3.5-turbo'\n",
    "\n",
    "client = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9hv9UvhuMOjnOM8S3OC4v80wcDYky', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None))], created=1720254824, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=GPT_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "    {'role': 'user', 'content': 'Who won the world series in 2020?'},\n",
    "    {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'},\n",
    "    {'role': 'user', 'content': 'Where was it played?'}\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print('Unable to generate ChatCompletion response')\n",
    "        print(f'Exception: {e}')\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model=GPT_MODEL):\n",
    "        self.model = model\n",
    "        self.memory = []\n",
    "        self.tools = []\n",
    "        \n",
    "    def invoke(self, message):\n",
    "        self.memory.append(\n",
    "            {'role': 'user', 'content': message}\n",
    "        )\n",
    "        chat_response = chat_completion_request(\n",
    "            messages=self.memory,\n",
    "            tools=self.tools,\n",
    "            model=self.model,\n",
    "        )\n",
    "\n",
    "        if chat_response.choices[0].finish_reason == 'stop':\n",
    "            chat_response_message = chat_response.choices[0].message.content\n",
    "            self.memory.append(\n",
    "                {'role': 'assistant', 'content': chat_response_message}\n",
    "            )\n",
    "            return chat_response_message\n",
    "\n",
    "        elif chat_response.choices[0].finish_reason == 'tool_calls':\n",
    "            # tool_calls = chat_response.choices[0].message.tool_calls\n",
    "            # for tool_call in tool_calls:\n",
    "            pass\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Relevant Files and Folders\n",
      "- README.md\n",
      "- assistant/tools.py\n",
      "- playground.ipynb\n",
      "- requirements.txt\n",
      "\n",
      "## File Contents\n",
      "\n",
      "### README.md\n",
      "```\n",
      "# AI Coding Assistant\n",
      " \n",
      "An OpenAI-powered assistant that will get actual code from your github repository.\n",
      "Then ask it questions and it will help you.\n",
      "\n",
      "## Benefits\n",
      "- No need to copy and paste snippets of your code since the assistant is able to retrieve your actual code\n",
      "- Holistic project understanding: the assistant can better assist with overall project development since it can see your whole repo\n",
      "\n",
      "## Weaknesses\n",
      "- This is still work in progress, I'll let you know\n",
      "```\n",
      "\n",
      "### assistant/tools.py\n",
      "```python\n",
      "import requests\n",
      "from typing import Dict, Any\n",
      "\n",
      "def fetch_github_repo(repo_url: str) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Fetches all files from a given GitHub repository URL and returns their content.\n",
      "    \"\"\"\n",
      "    # Extract owner and repo from the URL\n",
      "    parts = repo_url.split('/')\n",
      "    owner, repo = parts[-2], parts[-1]\n",
      "\n",
      "    # GitHub API URL to get the repo content\n",
      "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/\"\n",
      "\n",
      "    def get_contents(url):\n",
      "        response = requests.get(url)\n",
      "        response.raise_for_status()\n",
      "        return response.json()\n",
      "\n",
      "    def fetch_files(contents, path=\"\"):\n",
      "        files = {}\n",
      "        for item in contents:\n",
      "            item_path = f\"{path}/{item['name']}\" if path else item['name']\n",
      "            if item['type'] == 'file':\n",
      "                file_content = requests.get(item['download_url']).text\n",
      "                files[item_path] = file_content\n",
      "            elif item['type'] == 'dir':\n",
      "                dir_contents = get_contents(item['url'])\n",
      "                files.update(fetch_files(dir_contents, item_path))\n",
      "        return files\n",
      "\n",
      "    contents = get_contents(api_url)\n",
      "    repo_files = fetch_files(contents)\n",
      "\n",
      "    # Returning the collected data in JSON format\n",
      "    return repo_files\n",
      "```\n",
      "\n",
      "### playground.ipynb\n",
      "```python\n",
      "from dotenv import load_dotenv\n",
      "from openai import OpenAI\n",
      "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "GPT_MODEL = 'gpt-3.5-turbo'\n",
      "\n",
      "client = OpenAI()\n",
      "\n",
      "response = client.chat.completions.create(\n",
      "    model=GPT_MODEL,\n",
      "    messages=[\n",
      "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
      "        {'role': 'user', 'content': 'Who won the world series in 2020?'},\n",
      "        {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'},\n",
      "        {'role': 'user', 'content': 'Where was it played?'}\n",
      "    ]\n",
      ")\n",
      "\n",
      "response.choices[0].message.content\n",
      "\n",
      "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
      "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
      "    try:\n",
      "        response = client.chat.completions.create(\n",
      "            model=model,\n",
      "            messages=messages,\n",
      "            tools=tools,\n",
      "            tool_choice=tool_choice,\n",
      "        )\n",
      "        return response\n",
      "    except Exception as e:\n",
      "        print('Unable to generate ChatCompletion response')\n",
      "        print(f'Exception: {e}')\n",
      "        return e\n",
      "\n",
      "class Agent:\n",
      "    def __init__(self, model=GPT_MODEL):\n",
      "        self.model = model\n",
      "        self.memory = []\n",
      "        self.tools = []\n",
      "\n",
      "    def invoke(self, message):\n",
      "        self.memory.append(\n",
      "            {'role': 'user', 'content': message}\n",
      "        )\n",
      "        chat_response = chat_completion_request(\n",
      "            messages=self.memory,\n",
      "            tools=self.tools,\n",
      "            model=self.model,\n",
      "        )\n",
      "\n",
      "        if chat_response.choices[0].finish_reason == 'stop':\n",
      "            chat_response_message = chat_response.choices[0].message.content\n",
      "            self.memory.append(\n",
      "                {'role': 'assistant', 'content': chat_response_message}\n",
      "            )\n",
      "            return chat_response_message\n",
      "\n",
      "        elif chat_response.choices[0].finish_reason == 'tool_calls':\n",
      "            # to be implemented\n",
      "            pass\n",
      "\n",
      "agent = Agent()\n",
      "\n",
      "agent.memory.append(\n",
      "    {'role': 'system', 'content': 'You are a helpful assistant'}\n",
      ")\n",
      "\n",
      "while True:\n",
      "    user_input = input('User Message:')\n",
      "    if user_input == 'exit':\n",
      "        break\n",
      "    response = agent.invoke(user_input)\n",
      "    print('Assistant:', response)\n",
      "    print()\n",
      "```\n",
      "\n",
      "### requirements.txt\n",
      "```\n",
      "annotated-types==0.7.0\n",
      "anyio==4.4.0\n",
      "asttokens==2.4.1\n",
      "certifi==2024.7.4\n",
      "charset-normalizer==3.3.2\n",
      "colorama==0.4.6\n",
      "comm==0.2.2\n",
      "debugpy==1.8.2\n",
      "decorator==5.1.1\n",
      "distro==1.9.0\n",
      "exceptiongroup==1.2.1\n",
      "executing==2.0.1\n",
      "h11==0.14.0\n",
      "httpcore==1.0.5\n",
      "httpx==0.27.0\n",
      "idna==3.7\n",
      "ipykernel==6.29.5\n",
      "ipython==8.26.0\n",
      "jedi==0.19.1\n",
      "jupyter_client==8.6.2\n",
      "jupyter_core==5.7.2\n",
      "matplotlib-inline==0.1.7\n",
      "nest-asyncio==1.6.0\n",
      "openai==1.35.10\n",
      "packaging==24.1\n",
      "parso==0.8.4\n",
      "platformdirs==4.2.2\n",
      "prompt_toolkit==3.0.47\n",
      "psutil==6.0.0\n",
      "pure-eval==0.2.2\n",
      "pydantic==2.8.2\n",
      "pydantic_core==2.20.1\n",
      "Pygments==2.18.0\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.0.1\n",
      "pywin32==306\n",
      "pyzmq==26.0.3\n",
      "requests==2.32.3\n",
      "six==1.16.0\n",
      "sniffio==1.3.1\n",
      "stack-data==0.6.3\n",
      "tenacity==8.5.0\n",
      "tornado==6.4.1\n",
      "tqdm==4.66.4\n",
      "traitlets==5.14.3\n",
      "typing_extensions==4.12.2\n",
      "urllib3==2.2.2\n",
      "wcwidth==0.2.13\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "agent.memory.append(\n",
    "    {'role': 'system', 'content': '''You are an expert at reading code and also developing programs. You will be given some data from a github repo. \n",
    "    Primary Objectives: \n",
    "    1. You should identify only relevant files and folders \n",
    "    For example: .gitattributes, .gitignore might be irrelevant\n",
    "    2. Convert any notebook code (e.g. .ipynb) into more readable python\n",
    "    For example: ```{\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 10,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"from openai import OpenAI\\\\n\",\\n    \"from tenacity import retry, wait_random_exponential, stop_after_attempt\"\\n   ]\\n  }```\n",
    "    Should become: ``` from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt```\n",
    "    3. Once the above 2 steps have been done, give me output in the following format:\n",
    "    \n",
    "    \n",
    "     ## Repo Structure\n",
    "     (insert repo structure here)\n",
    "\n",
    "     ## File contents\n",
    "     file1.py\n",
    "     ```file1.py contents```\n",
    "\n",
    "     file2.ipynb\n",
    "     ```file2.ipynb contents```\n",
    "    \n",
    "'''}\n",
    ")\n",
    "\n",
    "agent.tools.append(\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'placeholder_tool',\n",
    "            'description': 'Placeholder tool not to be used',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'placeholder_property': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'Placeholder property',\n",
    "                    },\n",
    "                    'placeholder_property_2': {\n",
    "                        'type': 'integer',\n",
    "                        'description': 'Placeholder property 2'\n",
    "                    },\n",
    "                },\n",
    "                'required': ['placeholder_property', 'placeholder_property_2']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "repo_data = fetch_github_repo(\"https://github.com/cetyz/coding-ai\")\n",
    "\n",
    "response = agent.invoke(str(repo_data))\n",
    "print(response)\n",
    "\n",
    "# while True:\n",
    "#     user_input = input('User Message:')\n",
    "#     if user_input == 'exit':\n",
    "#         break\n",
    "#     response = agent.invoke(user_input)\n",
    "#     print('Assistant:', response)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.gitattributes': '# Auto detect text files and perform LF normalization\\n* text=auto\\n', '.gitignore': \"# Byte-compiled / optimized / DLL files\\n__pycache__/\\n*.py[cod]\\n*$py.class\\n\\n# C extensions\\n*.so\\n\\n# Distribution / packaging\\n.Python\\nbuild/\\ndevelop-eggs/\\ndist/\\ndownloads/\\neggs/\\n.eggs/\\nlib/\\nlib64/\\nparts/\\nsdist/\\nvar/\\nwheels/\\nshare/python-wheels/\\n*.egg-info/\\n.installed.cfg\\n*.egg\\nMANIFEST\\n\\n# PyInstaller\\n#  Usually these files are written by a python script from a template\\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\n*.manifest\\n*.spec\\n\\n# Installer logs\\npip-log.txt\\npip-delete-this-directory.txt\\n\\n# Unit test / coverage reports\\nhtmlcov/\\n.tox/\\n.nox/\\n.coverage\\n.coverage.*\\n.cache\\nnosetests.xml\\ncoverage.xml\\n*.cover\\n*.py,cover\\n.hypothesis/\\n.pytest_cache/\\ncover/\\n\\n# Translations\\n*.mo\\n*.pot\\n\\n# Django stuff:\\n*.log\\nlocal_settings.py\\ndb.sqlite3\\ndb.sqlite3-journal\\n\\n# Flask stuff:\\ninstance/\\n.webassets-cache\\n\\n# Scrapy stuff:\\n.scrapy\\n\\n# Sphinx documentation\\ndocs/_build/\\n\\n# PyBuilder\\n.pybuilder/\\ntarget/\\n\\n# Jupyter Notebook\\n.ipynb_checkpoints\\n\\n# IPython\\nprofile_default/\\nipython_config.py\\n\\n# pyenv\\n#   For a library or package, you might want to ignore these files since the code is\\n#   intended to run in multiple environments; otherwise, check them in:\\n# .python-version\\n\\n# pipenv\\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\\n#   install all needed dependencies.\\n#Pipfile.lock\\n\\n# poetry\\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\\n#   commonly ignored for libraries.\\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\\n#poetry.lock\\n\\n# pdm\\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\\n#pdm.lock\\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\\n#   in version control.\\n#   https://pdm.fming.dev/#use-with-ide\\n.pdm.toml\\n\\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\\n__pypackages__/\\n\\n# Celery stuff\\ncelerybeat-schedule\\ncelerybeat.pid\\n\\n# SageMath parsed files\\n*.sage.py\\n\\n# Environments\\n.env\\n.venv\\nenv/\\nvenv/\\nENV/\\nenv.bak/\\nvenv.bak/\\n\\n# Spyder project settings\\n.spyderproject\\n.spyproject\\n\\n# Rope project settings\\n.ropeproject\\n\\n# mkdocs documentation\\n/site\\n\\n# mypy\\n.mypy_cache/\\n.dmypy.json\\ndmypy.json\\n\\n# Pyre type checker\\n.pyre/\\n\\n# pytype static type analyzer\\n.pytype/\\n\\n# Cython debug symbols\\ncython_debug/\\n\\n# PyCharm\\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\\n#.idea/\\n\", 'README.md': \"# AI Coding Assistant\\n \\nAn OpenAI-powered assistant that will get actual code from your github repository.\\nThen ask it questions and it will help you.\\n\\n## Benefits\\n- No need to copy and paste snippets of your code since the assistant is able to retrieve your actual code\\n- Holistic project understanding: the assistant can better assist with overall project development since it can see your whole repo\\n\\n## Weaknesses\\n- This is still work in progress, I'll let you know\", 'playground.ipynb': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 10,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"from openai import OpenAI\\\\n\",\\n    \"from tenacity import retry, wait_random_exponential, stop_after_attempt\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 13,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"load_dotenv()\\\\n\",\\n    \"\\\\n\",\\n    \"GPT_MODEL = \\'gpt-3.5-turbo\\'\\\\n\",\\n    \"\\\\n\",\\n    \"client = OpenAI()\\\\n\",\\n    \"\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 15,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"ChatCompletion(id=\\'chatcmpl-9hv9UvhuMOjnOM8S3OC4v80wcDYky\\', choices=[Choice(finish_reason=\\'stop\\', index=0, logprobs=None, message=ChatCompletionMessage(content=\\'The 2020 World Series was played at Globe Life Field in Arlington, Texas.\\', role=\\'assistant\\', function_call=None, tool_calls=None))], created=1720254824, model=\\'gpt-3.5-turbo-0125\\', object=\\'chat.completion\\', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))\"\\n      ]\\n     },\\n     \"execution_count\": 15,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"response = client.chat.completions.create(\\\\n\",\\n    \"  model=GPT_MODEL,\\\\n\",\\n    \"  messages=[\\\\n\",\\n    \"    {\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistant.\\'},\\\\n\",\\n    \"    {\\'role\\': \\'user\\', \\'content\\': \\'Who won the world series in 2020?\\'},\\\\n\",\\n    \"    {\\'role\\': \\'assistant\\', \\'content\\': \\'The Los Angeles Dodgers won the World Series in 2020.\\'},\\\\n\",\\n    \"    {\\'role\\': \\'user\\', \\'content\\': \\'Where was it played?\\'}\\\\n\",\\n    \"  ]\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 25,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"\\'The 2020 World Series was played at Globe Life Field in Arlington, Texas.\\'\"\\n      ]\\n     },\\n     \"execution_count\": 25,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"response.choices[0].message.content\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 16,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\\\\n\",\\n    \"def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\\\\n\",\\n    \"    try:\\\\n\",\\n    \"        response = client.chat.completions.create(\\\\n\",\\n    \"            model=model,\\\\n\",\\n    \"            messages=messages,\\\\n\",\\n    \"            tools=tools,\\\\n\",\\n    \"            tool_choice=tool_choice,\\\\n\",\\n    \"        )\\\\n\",\\n    \"        return response\\\\n\",\\n    \"    except Exception as e:\\\\n\",\\n    \"        print(\\'Unable to generate ChatCompletion response\\')\\\\n\",\\n    \"        print(f\\'Exception: {e}\\')\\\\n\",\\n    \"        return e\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 26,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"class Agent:\\\\n\",\\n    \"    def __init__(self, model=GPT_MODEL):\\\\n\",\\n    \"        self.model = model\\\\n\",\\n    \"        self.memory = []\\\\n\",\\n    \"        self.tools = []\\\\n\",\\n    \"        \\\\n\",\\n    \"    def invoke(self, message):\\\\n\",\\n    \"        self.memory.append(\\\\n\",\\n    \"            {\\'role\\': \\'user\\', \\'content\\': message}\\\\n\",\\n    \"        )\\\\n\",\\n    \"        chat_response = chat_completion_request(\\\\n\",\\n    \"            messages=self.memory,\\\\n\",\\n    \"            tools=self.tools,\\\\n\",\\n    \"            model=self.model,\\\\n\",\\n    \"        )\\\\n\",\\n    \"\\\\n\",\\n    \"        if chat_response.choices[0].finish_reason == \\'stop\\':\\\\n\",\\n    \"            chat_response_message = chat_response.choices[0].message.content\\\\n\",\\n    \"            self.memory.append(\\\\n\",\\n    \"                {\\'role\\': \\'assistant\\', \\'content\\': chat_response_message}\\\\n\",\\n    \"            )\\\\n\",\\n    \"            return chat_response_message\\\\n\",\\n    \"\\\\n\",\\n    \"        elif chat_response.choices[0].finish_reason == \\'tool_calls\\':\\\\n\",\\n    \"            # to be implemented\\\\n\",\\n    \"            pass\\\\n\",\\n    \"\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 27,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Unable to generate ChatCompletion response\\\\n\",\\n      \"Exception: Error code: 400 - {\\'error\\': {\\'message\\': \\\\\"Invalid \\'tools\\': empty array. Expected an array with minimum length 1, but got an empty array instead.\\\\\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'tools\\', \\'code\\': \\'empty_array\\'}}\\\\n\"\\n     ]\\n    },\\n    {\\n     \"ename\": \"AttributeError\",\\n     \"evalue\": \"\\'BadRequestError\\' object has no attribute \\'choices\\'\",\\n     \"output_type\": \"error\",\\n     \"traceback\": [\\n      \"\\\\u001b[1;31m---------------------------------------------------------------------------\\\\u001b[0m\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m                            Traceback (most recent call last)\",\\n      \"Cell \\\\u001b[1;32mIn[27], line 11\\\\u001b[0m\\\\n\\\\u001b[0;32m      9\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m user_input \\\\u001b[38;5;241m==\\\\u001b[39m \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mexit\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m:\\\\n\\\\u001b[0;32m     10\\\\u001b[0m     \\\\u001b[38;5;28;01mbreak\\\\u001b[39;00m\\\\n\\\\u001b[1;32m---> 11\\\\u001b[0m response \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[43magent\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43minvoke\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\u001b[43muser_input\\\\u001b[49m\\\\u001b[43m)\\\\u001b[49m\\\\n\\\\u001b[0;32m     12\\\\u001b[0m \\\\u001b[38;5;28mprint\\\\u001b[39m(\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mAssistant:\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m, response)\\\\n\\\\u001b[0;32m     13\\\\u001b[0m \\\\u001b[38;5;28mprint\\\\u001b[39m()\\\\n\",\\n      \"Cell \\\\u001b[1;32mIn[26], line 17\\\\u001b[0m, in \\\\u001b[0;36mAgent.invoke\\\\u001b[1;34m(self, message)\\\\u001b[0m\\\\n\\\\u001b[0;32m      8\\\\u001b[0m \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mmemory\\\\u001b[38;5;241m.\\\\u001b[39mappend(\\\\n\\\\u001b[0;32m      9\\\\u001b[0m     {\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mrole\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m: \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124muser\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m, \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mcontent\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m: message}\\\\n\\\\u001b[0;32m     10\\\\u001b[0m )\\\\n\\\\u001b[0;32m     11\\\\u001b[0m chat_response \\\\u001b[38;5;241m=\\\\u001b[39m chat_completion_request(\\\\n\\\\u001b[0;32m     12\\\\u001b[0m     messages\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mmemory,\\\\n\\\\u001b[0;32m     13\\\\u001b[0m     tools\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mtools,\\\\n\\\\u001b[0;32m     14\\\\u001b[0m     model\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mmodel,\\\\n\\\\u001b[0;32m     15\\\\u001b[0m )\\\\n\\\\u001b[1;32m---> 17\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[43mchat_response\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mchoices\\\\u001b[49m[\\\\u001b[38;5;241m0\\\\u001b[39m]\\\\u001b[38;5;241m.\\\\u001b[39mfinish_reason \\\\u001b[38;5;241m==\\\\u001b[39m \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mstop\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m:\\\\n\\\\u001b[0;32m     18\\\\u001b[0m     chat_response_message \\\\u001b[38;5;241m=\\\\u001b[39m chat_response\\\\u001b[38;5;241m.\\\\u001b[39mchoices[\\\\u001b[38;5;241m0\\\\u001b[39m]\\\\u001b[38;5;241m.\\\\u001b[39mmessage\\\\u001b[38;5;241m.\\\\u001b[39mcontent\\\\n\\\\u001b[0;32m     19\\\\u001b[0m     \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mmemory\\\\u001b[38;5;241m.\\\\u001b[39mappend(\\\\n\\\\u001b[0;32m     20\\\\u001b[0m         {\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mrole\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m: \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124massistant\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m, \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mcontent\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m: chat_response_message}\\\\n\\\\u001b[0;32m     21\\\\u001b[0m     )\\\\n\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m: \\'BadRequestError\\' object has no attribute \\'choices\\'\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"agent = Agent()\\\\n\",\\n    \"\\\\n\",\\n    \"agent.memory.append(\\\\n\",\\n    \"    {\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistant.\\'}\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"while True:\\\\n\",\\n    \"    user_input = input(\\'User Message:\\')\\\\n\",\\n    \"    if user_input == \\'exit\\':\\\\n\",\\n    \"        break\\\\n\",\\n    \"    response = agent.invoke(user_input)\\\\n\",\\n    \"    print(\\'Assistant:\\', response)\\\\n\",\\n    \"    print()\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.10.2\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n', 'requirements.txt': 'annotated-types==0.7.0\\nanyio==4.4.0\\nasttokens==2.4.1\\ncertifi==2024.7.4\\ncharset-normalizer==3.3.2\\ncolorama==0.4.6\\ncomm==0.2.2\\ndebugpy==1.8.2\\ndecorator==5.1.1\\ndistro==1.9.0\\nexceptiongroup==1.2.1\\nexecuting==2.0.1\\nh11==0.14.0\\nhttpcore==1.0.5\\nhttpx==0.27.0\\nidna==3.7\\nipykernel==6.29.5\\nipython==8.26.0\\njedi==0.19.1\\njupyter_client==8.6.2\\njupyter_core==5.7.2\\nmatplotlib-inline==0.1.7\\nnest-asyncio==1.6.0\\nopenai==1.35.10\\npackaging==24.1\\nparso==0.8.4\\nplatformdirs==4.2.2\\nprompt_toolkit==3.0.47\\npsutil==6.0.0\\npure-eval==0.2.2\\npydantic==2.8.2\\npydantic_core==2.20.1\\nPygments==2.18.0\\npython-dateutil==2.9.0.post0\\npython-dotenv==1.0.1\\npywin32==306\\npyzmq==26.0.3\\nrequests==2.32.3\\nsix==1.16.0\\nsniffio==1.3.1\\nstack-data==0.6.3\\ntenacity==8.5.0\\ntornado==6.4.1\\ntqdm==4.66.4\\ntraitlets==5.14.3\\ntyping_extensions==4.12.2\\nurllib3==2.2.2\\nwcwidth==0.2.13\\n'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "repo_data = str(fetch_github_repo(\"https://github.com/cetyz/coding-ai\"))\n",
    "print(repo_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
