{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "from assistant.tools import fetch_github_repo\n",
    "from assistant.agents import Agent\n",
    "from assistant.prompts import github_cleaner_agent_system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'system',\n",
       " 'content': 'You are an expert at reading code and also developing programs. You will be given some data from a github repo. \\n    Primary Objectives: \\n    1. Convert any notebook code or other markdown (e.g. .ipynb) into more readable python\\n    For example: ```{\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 10,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"from openai import OpenAI\\\\n\",\\n    \"from tenacity import retry, wait_random_exponential, stop_after_attempt\"\\n   ]\\n  }```\\n    Should become: ``` from dotenv import load_dotenv\\nfrom openai import OpenAI\\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt```\\n    3. Then, give me output in the following format:\\n    \\n    \\n     ## Folder Structure\\n     (insert folder structure here)\\n\\n     ## File contents\\n     file1.py\\n     ```file1.py contents```\\n\\n     file2.ipynb\\n     ```file2.ipynb contents```\\n    \\n'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_cleaner_agent_system_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GPT_MODEL = 'gpt-3.5-turbo'\n",
    "\n",
    "client = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': 'Just saying hello!'}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    messages=messages,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9i4zRu6Btq006QpOCBxSp47HlLFMP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1720292641, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=11, total_tokens=20))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_cleaner_agent = Agent()\n",
    "\n",
    "github_cleaner_agent.memory.append(github_cleaner_agent_system_prompt())\n",
    "\n",
    "github_cleaner_agent.tools.append(\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'placeholder_tool',\n",
    "            'description': 'Placeholder tool not to be used',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'placeholder_property': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'Placeholder property',\n",
    "                    },\n",
    "                    'placeholder_property_2': {\n",
    "                        'type': 'integer',\n",
    "                        'description': 'Placeholder property 2'\n",
    "                    },\n",
    "                },\n",
    "                'required': ['placeholder_property', 'placeholder_property_2']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "repo_data = fetch_github_repo(\"https://github.com/cetyz/coding-ai\")\n",
    "\n",
    "github_response = github_cleaner_agent.invoke(str(repo_data))\n",
    "print(github_response)\n",
    "\n",
    "# while True:\n",
    "#     user_input = input('User Message:')\n",
    "#     if user_input == 'exit':\n",
    "#         break\n",
    "#     response = agent.invoke(user_input)\n",
    "#     print('Assistant:', response)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<framework>\n",
      "Summary of user question: User wants to make the AI Coding Assistant into an app using Streamlit and seeks guidance on how to do it and the recommended folder structure.\n",
      "Possible solutions to question: \n",
      "1. Create a Streamlit app that interacts with the existing code and API to provide a user interface for the AI Coding Assistant.\n",
      "2. Organize the files and folders in a structured manner to accommodate both the existing code and the new Streamlit app components.\n",
      "Given the current repo/code, shortlist solutions further: \n",
      "- Verify that the existing OpenAI and other dependencies are compatible with Streamlit.\n",
      "- Determine the specific features and interactions that the Streamlit app should provide.\n",
      "- Plan the integration method between the AI Coding Assistant and the new Streamlit app.\n",
      "Final answer: I recommend creating a new Python file for the Streamlit app and organizing it in a separate folder within the repository. You can leverage Streamlit's capabilities to create a simple and interactive interface for the AI Coding Assistant functions.\n",
      "Implementation of code: \n",
      "```python\n",
      "# streamlit_app.py\n",
      "import streamlit as st\n",
      "from assistant.agents import Agent\n",
      "\n",
      "# Initialize the Agent\n",
      "coding_assistant = Agent()\n",
      "\n",
      "# Streamlit UI\n",
      "st.title('AI Coding Assistant App')\n",
      "\n",
      "user_input = st.text_input('Ask a question:')\n",
      "if st.button('Get Answer'):\n",
      "    response = coding_assistant.invoke(user_input)\n",
      "    st.write('Assistant Response:')\n",
      "    st.write(response)\n",
      "```\n",
      "\n",
      "Explanation of implementation: \n",
      "- The code snippet creates a basic Streamlit app named `streamlit_app.py`.\n",
      "- It imports the `Agent` class from the existing code to interact with the AI Coding Assistant.\n",
      "- The Streamlit UI includes a text input for the user to ask questions and a button to trigger the Assistant response.\n",
      "- Upon clicking the button, the app invokes the Assistant with the user input and displays the response.\n",
      "</framework>\n"
     ]
    }
   ],
   "source": [
    "github_analyst_agent = Agent()\n",
    "\n",
    "github_analyst_agent.memory.append({\n",
    "        'role': 'system',\n",
    "        'content': f'''You are an expert at reading code and also developing programs. You will be given some data from a github repo. \n",
    "\n",
    "    Here is the github data: {github_response}\n",
    "\n",
    "    Primary Objectives: \n",
    "    1. Understand the repository and answer any questions the user may have\n",
    "    2. Think step by step how to answer this question\n",
    "    3. Give the final answer\n",
    "\n",
    "    Use the following framework for thinking and answering:\n",
    "    \n",
    "    Summary of user question: [summary]\n",
    "    Possible solutions to question: [possible solutions]\n",
    "    Given the current repo/code, shortlist solutions further: [shortlisted solutions]\n",
    "    Final answer: [choose solution, followed by reasoning]\n",
    "    Implementation of code: [```insert code or changes```]\n",
    "    Explanation of implementation: [explain written/edited code]\n",
    "     \n",
    "'''\n",
    "    })\n",
    "\n",
    "github_analyst_agent.tools.append(\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'placeholder_tool',\n",
    "            'description': 'Placeholder tool not to be used',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'placeholder_property': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'Placeholder property',\n",
    "                    },\n",
    "                    'placeholder_property_2': {\n",
    "                        'type': 'integer',\n",
    "                        'description': 'Placeholder property 2'\n",
    "                    },\n",
    "                },\n",
    "                'required': ['placeholder_property', 'placeholder_property_2']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "analysis_response = github_analyst_agent.invoke('I want to make this into an app using streamlit. How can I do so and what is the recommended folder structure?')\n",
    "print(analysis_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.gitattributes': '# Auto detect text files and perform LF normalization\\n* text=auto\\n', '.gitignore': \"# Byte-compiled / optimized / DLL files\\n__pycache__/\\n*.py[cod]\\n*$py.class\\n\\n# C extensions\\n*.so\\n\\n# Distribution / packaging\\n.Python\\nbuild/\\ndevelop-eggs/\\ndist/\\ndownloads/\\neggs/\\n.eggs/\\nlib/\\nlib64/\\nparts/\\nsdist/\\nvar/\\nwheels/\\nshare/python-wheels/\\n*.egg-info/\\n.installed.cfg\\n*.egg\\nMANIFEST\\n\\n# PyInstaller\\n#  Usually these files are written by a python script from a template\\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\n*.manifest\\n*.spec\\n\\n# Installer logs\\npip-log.txt\\npip-delete-this-directory.txt\\n\\n# Unit test / coverage reports\\nhtmlcov/\\n.tox/\\n.nox/\\n.coverage\\n.coverage.*\\n.cache\\nnosetests.xml\\ncoverage.xml\\n*.cover\\n*.py,cover\\n.hypothesis/\\n.pytest_cache/\\ncover/\\n\\n# Translations\\n*.mo\\n*.pot\\n\\n# Django stuff:\\n*.log\\nlocal_settings.py\\ndb.sqlite3\\ndb.sqlite3-journal\\n\\n# Flask stuff:\\ninstance/\\n.webassets-cache\\n\\n# Scrapy stuff:\\n.scrapy\\n\\n# Sphinx documentation\\ndocs/_build/\\n\\n# PyBuilder\\n.pybuilder/\\ntarget/\\n\\n# Jupyter Notebook\\n.ipynb_checkpoints\\n\\n# IPython\\nprofile_default/\\nipython_config.py\\n\\n# pyenv\\n#   For a library or package, you might want to ignore these files since the code is\\n#   intended to run in multiple environments; otherwise, check them in:\\n# .python-version\\n\\n# pipenv\\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\\n#   install all needed dependencies.\\n#Pipfile.lock\\n\\n# poetry\\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\\n#   commonly ignored for libraries.\\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\\n#poetry.lock\\n\\n# pdm\\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\\n#pdm.lock\\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\\n#   in version control.\\n#   https://pdm.fming.dev/#use-with-ide\\n.pdm.toml\\n\\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\\n__pypackages__/\\n\\n# Celery stuff\\ncelerybeat-schedule\\ncelerybeat.pid\\n\\n# SageMath parsed files\\n*.sage.py\\n\\n# Environments\\n.env\\n.venv\\nenv/\\nvenv/\\nENV/\\nenv.bak/\\nvenv.bak/\\n\\n# Spyder project settings\\n.spyderproject\\n.spyproject\\n\\n# Rope project settings\\n.ropeproject\\n\\n# mkdocs documentation\\n/site\\n\\n# mypy\\n.mypy_cache/\\n.dmypy.json\\ndmypy.json\\n\\n# Pyre type checker\\n.pyre/\\n\\n# pytype static type analyzer\\n.pytype/\\n\\n# Cython debug symbols\\ncython_debug/\\n\\n# PyCharm\\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\\n#.idea/\\n\", 'README.md': \"# AI Coding Assistant\\n \\nAn OpenAI-powered assistant that will get actual code from your github repository.\\nThen ask it questions and it will help you.\\n\\n## Benefits\\n- No need to copy and paste snippets of your code since the assistant is able to retrieve your actual code\\n- Holistic project understanding: the assistant can better assist with overall project development since it can see your whole repo\\n\\n## Weaknesses\\n- This is still work in progress, I'll let you know\", 'assistant/agents.py': \"from dotenv import load_dotenv\\nfrom openai import OpenAI\\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\\n\\nload_dotenv()\\n\\nGPT_MODEL = 'gpt-3.5-turbo'\\n\\nclient = OpenAI()\\n\\n@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\\ndef chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\\n    try:\\n        response = client.chat.completions.create(\\n            model=model,\\n            messages=messages,\\n            tools=tools,\\n            tool_choice=tool_choice,\\n        )\\n        return response\\n    except Exception as e:\\n        print('Unable to generate ChatCompletion response')\\n        print(f'Exception: {e}')\\n        return e\\n    \\n\\nclass Agent:\\n    def __init__(self, model=GPT_MODEL):\\n        self.model = model\\n        self.memory = []\\n        self.tools = []\\n        \\n    def invoke(self, message):\\n        self.memory.append(\\n            {'role': 'user', 'content': message}\\n        )\\n        chat_response = chat_completion_request(\\n            messages=self.memory,\\n            tools=self.tools,\\n            model=self.model,\\n        )\\n\\n        if chat_response.choices[0].finish_reason == 'stop':\\n            chat_response_message = chat_response.choices[0].message.content\\n            self.memory.append(\\n                {'role': 'assistant', 'content': chat_response_message}\\n            )\\n            return chat_response_message\\n\\n        elif chat_response.choices[0].finish_reason == 'tool_calls':\\n            # tool_calls = chat_response.choices[0].message.tool_calls\\n            # for tool_call in tool_calls:\\n            pass\", 'assistant/prompts.py': '', 'assistant/tools.py': 'import requests\\nfrom typing import Dict, Any\\n\\ndef fetch_github_repo(repo_url: str) -> Dict[str, Any]:\\n    \"\"\"\\n    Fetches all files from a given GitHub repository URL and returns their content.\\n    \"\"\"\\n    # Extract owner and repo from the URL\\n    parts = repo_url.split(\\'/\\')\\n    owner, repo = parts[-2], parts[-1]\\n\\n    # GitHub API URL to get the repo content\\n    api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/\"\\n\\n    def get_contents(url):\\n        response = requests.get(url)\\n        response.raise_for_status()\\n        return response.json()\\n\\n    def fetch_files(contents, path=\"\"):\\n        files = {}\\n        for item in contents:\\n            item_path = f\"{path}/{item[\\'name\\']}\" if path else item[\\'name\\']\\n            if item[\\'type\\'] == \\'file\\':\\n                file_content = requests.get(item[\\'download_url\\']).text\\n                files[item_path] = file_content\\n            elif item[\\'type\\'] == \\'dir\\':\\n                dir_contents = get_contents(item[\\'url\\'])\\n                files.update(fetch_files(dir_contents, item_path))\\n        return files\\n\\n    contents = get_contents(api_url)\\n    repo_files = fetch_files(contents)\\n\\n    # Returning the collected data in JSON format\\n    return repo_files', 'playground.ipynb': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 10,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"from openai import OpenAI\\\\n\",\\n    \"from tenacity import retry, wait_random_exponential, stop_after_attempt\\\\n\",\\n    \"\\\\n\",\\n    \"from assistant.tools import fetch_github_repo\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 13,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"load_dotenv()\\\\n\",\\n    \"\\\\n\",\\n    \"GPT_MODEL = \\'gpt-3.5-turbo\\'\\\\n\",\\n    \"\\\\n\",\\n    \"client = OpenAI()\\\\n\",\\n    \"\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 15,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"ChatCompletion(id=\\'chatcmpl-9hv9UvhuMOjnOM8S3OC4v80wcDYky\\', choices=[Choice(finish_reason=\\'stop\\', index=0, logprobs=None, message=ChatCompletionMessage(content=\\'The 2020 World Series was played at Globe Life Field in Arlington, Texas.\\', role=\\'assistant\\', function_call=None, tool_calls=None))], created=1720254824, model=\\'gpt-3.5-turbo-0125\\', object=\\'chat.completion\\', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))\"\\n      ]\\n     },\\n     \"execution_count\": 15,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"response = client.chat.completions.create(\\\\n\",\\n    \"  model=GPT_MODEL,\\\\n\",\\n    \"  messages=[\\\\n\",\\n    \"    {\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistant.\\'},\\\\n\",\\n    \"    {\\'role\\': \\'user\\', \\'content\\': \\'Who won the world series in 2020?\\'},\\\\n\",\\n    \"    {\\'role\\': \\'assistant\\', \\'content\\': \\'The Los Angeles Dodgers won the World Series in 2020.\\'},\\\\n\",\\n    \"    {\\'role\\': \\'user\\', \\'content\\': \\'Where was it played?\\'}\\\\n\",\\n    \"  ]\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 25,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"\\'The 2020 World Series was played at Globe Life Field in Arlington, Texas.\\'\"\\n      ]\\n     },\\n     \"execution_count\": 25,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"response.choices[0].message.content\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 31,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\\\\n\",\\n    \"def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\\\\n\",\\n    \"    try:\\\\n\",\\n    \"        response = client.chat.completions.create(\\\\n\",\\n    \"            model=model,\\\\n\",\\n    \"            messages=messages,\\\\n\",\\n    \"            tools=tools,\\\\n\",\\n    \"            tool_choice=tool_choice,\\\\n\",\\n    \"        )\\\\n\",\\n    \"        return response\\\\n\",\\n    \"    except Exception as e:\\\\n\",\\n    \"        print(\\'Unable to generate ChatCompletion response\\')\\\\n\",\\n    \"        print(f\\'Exception: {e}\\')\\\\n\",\\n    \"        return e\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 32,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"class Agent:\\\\n\",\\n    \"    def __init__(self, model=GPT_MODEL):\\\\n\",\\n    \"        self.model = model\\\\n\",\\n    \"        self.memory = []\\\\n\",\\n    \"        self.tools = []\\\\n\",\\n    \"        \\\\n\",\\n    \"    def invoke(self, message):\\\\n\",\\n    \"        self.memory.append(\\\\n\",\\n    \"            {\\'role\\': \\'user\\', \\'content\\': message}\\\\n\",\\n    \"        )\\\\n\",\\n    \"        chat_response = chat_completion_request(\\\\n\",\\n    \"            messages=self.memory,\\\\n\",\\n    \"            tools=self.tools,\\\\n\",\\n    \"            model=self.model,\\\\n\",\\n    \"        )\\\\n\",\\n    \"\\\\n\",\\n    \"        if chat_response.choices[0].finish_reason == \\'stop\\':\\\\n\",\\n    \"            chat_response_message = chat_response.choices[0].message.content\\\\n\",\\n    \"            self.memory.append(\\\\n\",\\n    \"                {\\'role\\': \\'assistant\\', \\'content\\': chat_response_message}\\\\n\",\\n    \"            )\\\\n\",\\n    \"            return chat_response_message\\\\n\",\\n    \"\\\\n\",\\n    \"        elif chat_response.choices[0].finish_reason == \\'tool_calls\\':\\\\n\",\\n    \"            # tool_calls = chat_response.choices[0].message.tool_calls\\\\n\",\\n    \"            # for tool_call in tool_calls:\\\\n\",\\n    \"            pass\\\\n\",\\n    \"                \\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 37,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"## Relevant Files and Folders\\\\n\",\\n      \"- README.md\\\\n\",\\n      \"- assistant/tools.py\\\\n\",\\n      \"- playground.ipynb\\\\n\",\\n      \"- requirements.txt\\\\n\",\\n      \"\\\\n\",\\n      \"## File Contents\\\\n\",\\n      \"\\\\n\",\\n      \"### README.md\\\\n\",\\n      \"```\\\\n\",\\n      \"# AI Coding Assistant\\\\n\",\\n      \" \\\\n\",\\n      \"An OpenAI-powered assistant that will get actual code from your github repository.\\\\n\",\\n      \"Then ask it questions and it will help you.\\\\n\",\\n      \"\\\\n\",\\n      \"## Benefits\\\\n\",\\n      \"- No need to copy and paste snippets of your code since the assistant is able to retrieve your actual code\\\\n\",\\n      \"- Holistic project understanding: the assistant can better assist with overall project development since it can see your whole repo\\\\n\",\\n      \"\\\\n\",\\n      \"## Weaknesses\\\\n\",\\n      \"- This is still work in progress, I\\'ll let you know\\\\n\",\\n      \"```\\\\n\",\\n      \"\\\\n\",\\n      \"### assistant/tools.py\\\\n\",\\n      \"```python\\\\n\",\\n      \"import requests\\\\n\",\\n      \"from typing import Dict, Any\\\\n\",\\n      \"\\\\n\",\\n      \"def fetch_github_repo(repo_url: str) -> Dict[str, Any]:\\\\n\",\\n      \"    \\\\\"\\\\\"\\\\\"\\\\n\",\\n      \"    Fetches all files from a given GitHub repository URL and returns their content.\\\\n\",\\n      \"    \\\\\"\\\\\"\\\\\"\\\\n\",\\n      \"    # Extract owner and repo from the URL\\\\n\",\\n      \"    parts = repo_url.split(\\'/\\')\\\\n\",\\n      \"    owner, repo = parts[-2], parts[-1]\\\\n\",\\n      \"\\\\n\",\\n      \"    # GitHub API URL to get the repo content\\\\n\",\\n      \"    api_url = f\\\\\"https://api.github.com/repos/{owner}/{repo}/contents/\\\\\"\\\\n\",\\n      \"\\\\n\",\\n      \"    def get_contents(url):\\\\n\",\\n      \"        response = requests.get(url)\\\\n\",\\n      \"        response.raise_for_status()\\\\n\",\\n      \"        return response.json()\\\\n\",\\n      \"\\\\n\",\\n      \"    def fetch_files(contents, path=\\\\\"\\\\\"):\\\\n\",\\n      \"        files = {}\\\\n\",\\n      \"        for item in contents:\\\\n\",\\n      \"            item_path = f\\\\\"{path}/{item[\\'name\\']}\\\\\" if path else item[\\'name\\']\\\\n\",\\n      \"            if item[\\'type\\'] == \\'file\\':\\\\n\",\\n      \"                file_content = requests.get(item[\\'download_url\\']).text\\\\n\",\\n      \"                files[item_path] = file_content\\\\n\",\\n      \"            elif item[\\'type\\'] == \\'dir\\':\\\\n\",\\n      \"                dir_contents = get_contents(item[\\'url\\'])\\\\n\",\\n      \"                files.update(fetch_files(dir_contents, item_path))\\\\n\",\\n      \"        return files\\\\n\",\\n      \"\\\\n\",\\n      \"    contents = get_contents(api_url)\\\\n\",\\n      \"    repo_files = fetch_files(contents)\\\\n\",\\n      \"\\\\n\",\\n      \"    # Returning the collected data in JSON format\\\\n\",\\n      \"    return repo_files\\\\n\",\\n      \"```\\\\n\",\\n      \"\\\\n\",\\n      \"### playground.ipynb\\\\n\",\\n      \"```python\\\\n\",\\n      \"from dotenv import load_dotenv\\\\n\",\\n      \"from openai import OpenAI\\\\n\",\\n      \"from tenacity import retry, wait_random_exponential, stop_after_attempt\\\\n\",\\n      \"\\\\n\",\\n      \"load_dotenv()\\\\n\",\\n      \"\\\\n\",\\n      \"GPT_MODEL = \\'gpt-3.5-turbo\\'\\\\n\",\\n      \"\\\\n\",\\n      \"client = OpenAI()\\\\n\",\\n      \"\\\\n\",\\n      \"response = client.chat.completions.create(\\\\n\",\\n      \"    model=GPT_MODEL,\\\\n\",\\n      \"    messages=[\\\\n\",\\n      \"        {\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistant.\\'},\\\\n\",\\n      \"        {\\'role\\': \\'user\\', \\'content\\': \\'Who won the world series in 2020?\\'},\\\\n\",\\n      \"        {\\'role\\': \\'assistant\\', \\'content\\': \\'The Los Angeles Dodgers won the World Series in 2020.\\'},\\\\n\",\\n      \"        {\\'role\\': \\'user\\', \\'content\\': \\'Where was it played?\\'}\\\\n\",\\n      \"    ]\\\\n\",\\n      \")\\\\n\",\\n      \"\\\\n\",\\n      \"response.choices[0].message.content\\\\n\",\\n      \"\\\\n\",\\n      \"@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\\\\n\",\\n      \"def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\\\\n\",\\n      \"    try:\\\\n\",\\n      \"        response = client.chat.completions.create(\\\\n\",\\n      \"            model=model,\\\\n\",\\n      \"            messages=messages,\\\\n\",\\n      \"            tools=tools,\\\\n\",\\n      \"            tool_choice=tool_choice,\\\\n\",\\n      \"        )\\\\n\",\\n      \"        return response\\\\n\",\\n      \"    except Exception as e:\\\\n\",\\n      \"        print(\\'Unable to generate ChatCompletion response\\')\\\\n\",\\n      \"        print(f\\'Exception: {e}\\')\\\\n\",\\n      \"        return e\\\\n\",\\n      \"\\\\n\",\\n      \"class Agent:\\\\n\",\\n      \"    def __init__(self, model=GPT_MODEL):\\\\n\",\\n      \"        self.model = model\\\\n\",\\n      \"        self.memory = []\\\\n\",\\n      \"        self.tools = []\\\\n\",\\n      \"\\\\n\",\\n      \"    def invoke(self, message):\\\\n\",\\n      \"        self.memory.append(\\\\n\",\\n      \"            {\\'role\\': \\'user\\', \\'content\\': message}\\\\n\",\\n      \"        )\\\\n\",\\n      \"        chat_response = chat_completion_request(\\\\n\",\\n      \"            messages=self.memory,\\\\n\",\\n      \"            tools=self.tools,\\\\n\",\\n      \"            model=self.model,\\\\n\",\\n      \"        )\\\\n\",\\n      \"\\\\n\",\\n      \"        if chat_response.choices[0].finish_reason == \\'stop\\':\\\\n\",\\n      \"            chat_response_message = chat_response.choices[0].message.content\\\\n\",\\n      \"            self.memory.append(\\\\n\",\\n      \"                {\\'role\\': \\'assistant\\', \\'content\\': chat_response_message}\\\\n\",\\n      \"            )\\\\n\",\\n      \"            return chat_response_message\\\\n\",\\n      \"\\\\n\",\\n      \"        elif chat_response.choices[0].finish_reason == \\'tool_calls\\':\\\\n\",\\n      \"            # to be implemented\\\\n\",\\n      \"            pass\\\\n\",\\n      \"\\\\n\",\\n      \"agent = Agent()\\\\n\",\\n      \"\\\\n\",\\n      \"agent.memory.append(\\\\n\",\\n      \"    {\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistant\\'}\\\\n\",\\n      \")\\\\n\",\\n      \"\\\\n\",\\n      \"while True:\\\\n\",\\n      \"    user_input = input(\\'User Message:\\')\\\\n\",\\n      \"    if user_input == \\'exit\\':\\\\n\",\\n      \"        break\\\\n\",\\n      \"    response = agent.invoke(user_input)\\\\n\",\\n      \"    print(\\'Assistant:\\', response)\\\\n\",\\n      \"    print()\\\\n\",\\n      \"```\\\\n\",\\n      \"\\\\n\",\\n      \"### requirements.txt\\\\n\",\\n      \"```\\\\n\",\\n      \"annotated-types==0.7.0\\\\n\",\\n      \"anyio==4.4.0\\\\n\",\\n      \"asttokens==2.4.1\\\\n\",\\n      \"certifi==2024.7.4\\\\n\",\\n      \"charset-normalizer==3.3.2\\\\n\",\\n      \"colorama==0.4.6\\\\n\",\\n      \"comm==0.2.2\\\\n\",\\n      \"debugpy==1.8.2\\\\n\",\\n      \"decorator==5.1.1\\\\n\",\\n      \"distro==1.9.0\\\\n\",\\n      \"exceptiongroup==1.2.1\\\\n\",\\n      \"executing==2.0.1\\\\n\",\\n      \"h11==0.14.0\\\\n\",\\n      \"httpcore==1.0.5\\\\n\",\\n      \"httpx==0.27.0\\\\n\",\\n      \"idna==3.7\\\\n\",\\n      \"ipykernel==6.29.5\\\\n\",\\n      \"ipython==8.26.0\\\\n\",\\n      \"jedi==0.19.1\\\\n\",\\n      \"jupyter_client==8.6.2\\\\n\",\\n      \"jupyter_core==5.7.2\\\\n\",\\n      \"matplotlib-inline==0.1.7\\\\n\",\\n      \"nest-asyncio==1.6.0\\\\n\",\\n      \"openai==1.35.10\\\\n\",\\n      \"packaging==24.1\\\\n\",\\n      \"parso==0.8.4\\\\n\",\\n      \"platformdirs==4.2.2\\\\n\",\\n      \"prompt_toolkit==3.0.47\\\\n\",\\n      \"psutil==6.0.0\\\\n\",\\n      \"pure-eval==0.2.2\\\\n\",\\n      \"pydantic==2.8.2\\\\n\",\\n      \"pydantic_core==2.20.1\\\\n\",\\n      \"Pygments==2.18.0\\\\n\",\\n      \"python-dateutil==2.9.0.post0\\\\n\",\\n      \"python-dotenv==1.0.1\\\\n\",\\n      \"pywin32==306\\\\n\",\\n      \"pyzmq==26.0.3\\\\n\",\\n      \"requests==2.32.3\\\\n\",\\n      \"six==1.16.0\\\\n\",\\n      \"sniffio==1.3.1\\\\n\",\\n      \"stack-data==0.6.3\\\\n\",\\n      \"tenacity==8.5.0\\\\n\",\\n      \"tornado==6.4.1\\\\n\",\\n      \"tqdm==4.66.4\\\\n\",\\n      \"traitlets==5.14.3\\\\n\",\\n      \"typing_extensions==4.12.2\\\\n\",\\n      \"urllib3==2.2.2\\\\n\",\\n      \"wcwidth==0.2.13\\\\n\",\\n      \"```\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"agent = Agent()\\\\n\",\\n    \"\\\\n\",\\n    \"agent.memory.append(\\\\n\",\\n    \"    {\\'role\\': \\'system\\', \\'content\\': \\'\\'\\'You are an expert at reading code and also developing programs. You will be given some data from a github repo. \\\\n\",\\n    \"    Primary Objectives: \\\\n\",\\n    \"    1. You should identify only relevant files and folders \\\\n\",\\n    \"    For example: .gitattributes, .gitignore might be irrelevant\\\\n\",\\n    \"    2. Convert any notebook code (e.g. .ipynb) into more readable python\\\\n\",\\n    \"    For example: ```{\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": 10,\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"from dotenv import load_dotenv\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"from openai import OpenAI\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"from tenacity import retry, wait_random_exponential, stop_after_attempt\\\\\"\\\\\\\\n   ]\\\\\\\\n  }```\\\\n\",\\n    \"    Should become: ``` from dotenv import load_dotenv\\\\n\",\\n    \"from openai import OpenAI\\\\n\",\\n    \"from tenacity import retry, wait_random_exponential, stop_after_attempt```\\\\n\",\\n    \"    3. Once the above 2 steps have been done, give me output in the following format:\\\\n\",\\n    \"    \\\\n\",\\n    \"    \\\\n\",\\n    \"     ## Repo Structure\\\\n\",\\n    \"     (insert repo structure here)\\\\n\",\\n    \"\\\\n\",\\n    \"     ## File contents\\\\n\",\\n    \"     file1.py\\\\n\",\\n    \"     ```file1.py contents```\\\\n\",\\n    \"\\\\n\",\\n    \"     file2.ipynb\\\\n\",\\n    \"     ```file2.ipynb contents```\\\\n\",\\n    \"    \\\\n\",\\n    \"\\'\\'\\'}\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"agent.tools.append(\\\\n\",\\n    \"    {\\\\n\",\\n    \"        \\'type\\': \\'function\\',\\\\n\",\\n    \"        \\'function\\': {\\\\n\",\\n    \"            \\'name\\': \\'placeholder_tool\\',\\\\n\",\\n    \"            \\'description\\': \\'Placeholder tool not to be used\\',\\\\n\",\\n    \"            \\'parameters\\': {\\\\n\",\\n    \"                \\'type\\': \\'object\\',\\\\n\",\\n    \"                \\'properties\\': {\\\\n\",\\n    \"                    \\'placeholder_property\\': {\\\\n\",\\n    \"                        \\'type\\': \\'string\\',\\\\n\",\\n    \"                        \\'description\\': \\'Placeholder property\\',\\\\n\",\\n    \"                    },\\\\n\",\\n    \"                    \\'placeholder_property_2\\': {\\\\n\",\\n    \"                        \\'type\\': \\'integer\\',\\\\n\",\\n    \"                        \\'description\\': \\'Placeholder property 2\\'\\\\n\",\\n    \"                    },\\\\n\",\\n    \"                },\\\\n\",\\n    \"                \\'required\\': [\\'placeholder_property\\', \\'placeholder_property_2\\']\\\\n\",\\n    \"            }\\\\n\",\\n    \"        }\\\\n\",\\n    \"    }\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"repo_data = fetch_github_repo(\\\\\"https://github.com/cetyz/coding-ai\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"response = agent.invoke(str(repo_data))\\\\n\",\\n    \"print(response)\\\\n\",\\n    \"\\\\n\",\\n    \"# while True:\\\\n\",\\n    \"#     user_input = input(\\'User Message:\\')\\\\n\",\\n    \"#     if user_input == \\'exit\\':\\\\n\",\\n    \"#         break\\\\n\",\\n    \"#     response = agent.invoke(user_input)\\\\n\",\\n    \"#     print(\\'Assistant:\\', response)\\\\n\",\\n    \"#     print()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 34,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"{\\'.gitattributes\\': \\'# Auto detect text files and perform LF normalization\\\\\\\\n* text=auto\\\\\\\\n\\', \\'.gitignore\\': \\\\\"# Byte-compiled / optimized / DLL files\\\\\\\\n__pycache__/\\\\\\\\n*.py[cod]\\\\\\\\n*$py.class\\\\\\\\n\\\\\\\\n# C extensions\\\\\\\\n*.so\\\\\\\\n\\\\\\\\n# Distribution / packaging\\\\\\\\n.Python\\\\\\\\nbuild/\\\\\\\\ndevelop-eggs/\\\\\\\\ndist/\\\\\\\\ndownloads/\\\\\\\\neggs/\\\\\\\\n.eggs/\\\\\\\\nlib/\\\\\\\\nlib64/\\\\\\\\nparts/\\\\\\\\nsdist/\\\\\\\\nvar/\\\\\\\\nwheels/\\\\\\\\nshare/python-wheels/\\\\\\\\n*.egg-info/\\\\\\\\n.installed.cfg\\\\\\\\n*.egg\\\\\\\\nMANIFEST\\\\\\\\n\\\\\\\\n# PyInstaller\\\\\\\\n#  Usually these files are written by a python script from a template\\\\\\\\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\\\\\\\n*.manifest\\\\\\\\n*.spec\\\\\\\\n\\\\\\\\n# Installer logs\\\\\\\\npip-log.txt\\\\\\\\npip-delete-this-directory.txt\\\\\\\\n\\\\\\\\n# Unit test / coverage reports\\\\\\\\nhtmlcov/\\\\\\\\n.tox/\\\\\\\\n.nox/\\\\\\\\n.coverage\\\\\\\\n.coverage.*\\\\\\\\n.cache\\\\\\\\nnosetests.xml\\\\\\\\ncoverage.xml\\\\\\\\n*.cover\\\\\\\\n*.py,cover\\\\\\\\n.hypothesis/\\\\\\\\n.pytest_cache/\\\\\\\\ncover/\\\\\\\\n\\\\\\\\n# Translations\\\\\\\\n*.mo\\\\\\\\n*.pot\\\\\\\\n\\\\\\\\n# Django stuff:\\\\\\\\n*.log\\\\\\\\nlocal_settings.py\\\\\\\\ndb.sqlite3\\\\\\\\ndb.sqlite3-journal\\\\\\\\n\\\\\\\\n# Flask stuff:\\\\\\\\ninstance/\\\\\\\\n.webassets-cache\\\\\\\\n\\\\\\\\n# Scrapy stuff:\\\\\\\\n.scrapy\\\\\\\\n\\\\\\\\n# Sphinx documentation\\\\\\\\ndocs/_build/\\\\\\\\n\\\\\\\\n# PyBuilder\\\\\\\\n.pybuilder/\\\\\\\\ntarget/\\\\\\\\n\\\\\\\\n# Jupyter Notebook\\\\\\\\n.ipynb_checkpoints\\\\\\\\n\\\\\\\\n# IPython\\\\\\\\nprofile_default/\\\\\\\\nipython_config.py\\\\\\\\n\\\\\\\\n# pyenv\\\\\\\\n#   For a library or package, you might want to ignore these files since the code is\\\\\\\\n#   intended to run in multiple environments; otherwise, check them in:\\\\\\\\n# .python-version\\\\\\\\n\\\\\\\\n# pipenv\\\\\\\\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\\\\\\\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\\\\\\\n#   having no cross-platform support, pipenv may install dependencies that don\\'t work, or not\\\\\\\\n#   install all needed dependencies.\\\\\\\\n#Pipfile.lock\\\\\\\\n\\\\\\\\n# poetry\\\\\\\\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\\\\\\\\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\\\\\\\\n#   commonly ignored for libraries.\\\\\\\\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\\\\\\\\n#poetry.lock\\\\\\\\n\\\\\\\\n# pdm\\\\\\\\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\\\\\\\\n#pdm.lock\\\\\\\\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\\\\\\\\n#   in version control.\\\\\\\\n#   https://pdm.fming.dev/#use-with-ide\\\\\\\\n.pdm.toml\\\\\\\\n\\\\\\\\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\\\\\\\\n__pypackages__/\\\\\\\\n\\\\\\\\n# Celery stuff\\\\\\\\ncelerybeat-schedule\\\\\\\\ncelerybeat.pid\\\\\\\\n\\\\\\\\n# SageMath parsed files\\\\\\\\n*.sage.py\\\\\\\\n\\\\\\\\n# Environments\\\\\\\\n.env\\\\\\\\n.venv\\\\\\\\nenv/\\\\\\\\nvenv/\\\\\\\\nENV/\\\\\\\\nenv.bak/\\\\\\\\nvenv.bak/\\\\\\\\n\\\\\\\\n# Spyder project settings\\\\\\\\n.spyderproject\\\\\\\\n.spyproject\\\\\\\\n\\\\\\\\n# Rope project settings\\\\\\\\n.ropeproject\\\\\\\\n\\\\\\\\n# mkdocs documentation\\\\\\\\n/site\\\\\\\\n\\\\\\\\n# mypy\\\\\\\\n.mypy_cache/\\\\\\\\n.dmypy.json\\\\\\\\ndmypy.json\\\\\\\\n\\\\\\\\n# Pyre type checker\\\\\\\\n.pyre/\\\\\\\\n\\\\\\\\n# pytype static type analyzer\\\\\\\\n.pytype/\\\\\\\\n\\\\\\\\n# Cython debug symbols\\\\\\\\ncython_debug/\\\\\\\\n\\\\\\\\n# PyCharm\\\\\\\\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\\\\\\\\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\\\\\\\\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\\\\\\\\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\\\\\\\\n#.idea/\\\\\\\\n\\\\\", \\'README.md\\': \\\\\"# AI Coding Assistant\\\\\\\\n \\\\\\\\nAn OpenAI-powered assistant that will get actual code from your github repository.\\\\\\\\nThen ask it questions and it will help you.\\\\\\\\n\\\\\\\\n## Benefits\\\\\\\\n- No need to copy and paste snippets of your code since the assistant is able to retrieve your actual code\\\\\\\\n- Holistic project understanding: the assistant can better assist with overall project development since it can see your whole repo\\\\\\\\n\\\\\\\\n## Weaknesses\\\\\\\\n- This is still work in progress, I\\'ll let you know\\\\\", \\'playground.ipynb\\': \\'{\\\\\\\\n \\\\\"cells\\\\\": [\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": 10,\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"from dotenv import load_dotenv\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"from openai import OpenAI\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"from tenacity import retry, wait_random_exponential, stop_after_attempt\\\\\"\\\\\\\\n   ]\\\\\\\\n  },\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": 13,\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"load_dotenv()\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"GPT_MODEL = \\\\\\\\\\'gpt-3.5-turbo\\\\\\\\\\'\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"client = OpenAI()\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\"\\\\\\\\n   ]\\\\\\\\n  },\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": 15,\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [\\\\\\\\n    {\\\\\\\\n     \\\\\"data\\\\\": {\\\\\\\\n      \\\\\"text/plain\\\\\": [\\\\\\\\n       \\\\\"ChatCompletion(id=\\\\\\\\\\'chatcmpl-9hv9UvhuMOjnOM8S3OC4v80wcDYky\\\\\\\\\\', choices=[Choice(finish_reason=\\\\\\\\\\'stop\\\\\\\\\\', index=0, logprobs=None, message=ChatCompletionMessage(content=\\\\\\\\\\'The 2020 World Series was played at Globe Life Field in Arlington, Texas.\\\\\\\\\\', role=\\\\\\\\\\'assistant\\\\\\\\\\', function_call=None, tool_calls=None))], created=1720254824, model=\\\\\\\\\\'gpt-3.5-turbo-0125\\\\\\\\\\', object=\\\\\\\\\\'chat.completion\\\\\\\\\\', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))\\\\\"\\\\\\\\n      ]\\\\\\\\n     },\\\\\\\\n     \\\\\"execution_count\\\\\": 15,\\\\\\\\n     \\\\\"metadata\\\\\": {},\\\\\\\\n     \\\\\"output_type\\\\\": \\\\\"execute_result\\\\\"\\\\\\\\n    }\\\\\\\\n   ],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"response = client.chat.completions.create(\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"  model=GPT_MODEL,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"  messages=[\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    {\\\\\\\\\\'role\\\\\\\\\\': \\\\\\\\\\'system\\\\\\\\\\', \\\\\\\\\\'content\\\\\\\\\\': \\\\\\\\\\'You are a helpful assistant.\\\\\\\\\\'},\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    {\\\\\\\\\\'role\\\\\\\\\\': \\\\\\\\\\'user\\\\\\\\\\', \\\\\\\\\\'content\\\\\\\\\\': \\\\\\\\\\'Who won the world series in 2020?\\\\\\\\\\'},\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    {\\\\\\\\\\'role\\\\\\\\\\': \\\\\\\\\\'assistant\\\\\\\\\\', \\\\\\\\\\'content\\\\\\\\\\': \\\\\\\\\\'The Los Angeles Dodgers won the World Series in 2020.\\\\\\\\\\'},\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    {\\\\\\\\\\'role\\\\\\\\\\': \\\\\\\\\\'user\\\\\\\\\\', \\\\\\\\\\'content\\\\\\\\\\': \\\\\\\\\\'Where was it played?\\\\\\\\\\'}\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"  ]\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\")\\\\\\\\\\\\\\\\n\\\\\"\\\\\\\\n   ]\\\\\\\\n  },\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": 25,\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [\\\\\\\\n    {\\\\\\\\n     \\\\\"data\\\\\": {\\\\\\\\n      \\\\\"text/plain\\\\\": [\\\\\\\\n       \\\\\"\\\\\\\\\\'The 2020 World Series was played at Globe Life Field in Arlington, Texas.\\\\\\\\\\'\\\\\"\\\\\\\\n      ]\\\\\\\\n     },\\\\\\\\n     \\\\\"execution_count\\\\\": 25,\\\\\\\\n     \\\\\"metadata\\\\\": {},\\\\\\\\n     \\\\\"output_type\\\\\": \\\\\"execute_result\\\\\"\\\\\\\\n    }\\\\\\\\n   ],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"response.choices[0].message.content\\\\\"\\\\\\\\n   ]\\\\\\\\n  },\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": 16,\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    try:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        response = client.chat.completions.create(\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            model=model,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            messages=messages,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            tools=tools,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            tool_choice=tool_choice,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        )\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        return response\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    except Exception as e:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        print(\\\\\\\\\\'Unable to generate ChatCompletion response\\\\\\\\\\')\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        print(f\\\\\\\\\\'Exception: {e}\\\\\\\\\\')\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        return e\\\\\"\\\\\\\\n   ]\\\\\\\\n  },\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": 26,\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"class Agent:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    def __init__(self, model=GPT_MODEL):\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        self.model = model\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        self.memory = []\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        self.tools = []\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        \\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    def invoke(self, message):\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        self.memory.append(\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            {\\\\\\\\\\'role\\\\\\\\\\': \\\\\\\\\\'user\\\\\\\\\\', \\\\\\\\\\'content\\\\\\\\\\': message}\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        )\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        chat_response = chat_completion_request(\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            messages=self.memory,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            tools=self.tools,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            model=self.model,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        )\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        if chat_response.choices[0].finish_reason == \\\\\\\\\\'stop\\\\\\\\\\':\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            chat_response_message = chat_response.choices[0].message.content\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            self.memory.append(\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"                {\\\\\\\\\\'role\\\\\\\\\\': \\\\\\\\\\'assistant\\\\\\\\\\', \\\\\\\\\\'content\\\\\\\\\\': chat_response_message}\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            )\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            return chat_response_message\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        elif chat_response.choices[0].finish_reason == \\\\\\\\\\'tool_calls\\\\\\\\\\':\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            # to be implemented\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"            pass\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\"\\\\\\\\n   ]\\\\\\\\n  },\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": 27,\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [\\\\\\\\n    {\\\\\\\\n     \\\\\"name\\\\\": \\\\\"stdout\\\\\",\\\\\\\\n     \\\\\"output_type\\\\\": \\\\\"stream\\\\\",\\\\\\\\n     \\\\\"text\\\\\": [\\\\\\\\n      \\\\\"Unable to generate ChatCompletion response\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n      \\\\\"Exception: Error code: 400 - {\\\\\\\\\\'error\\\\\\\\\\': {\\\\\\\\\\'message\\\\\\\\\\': \\\\\\\\\\\\\\\\\\\\\"Invalid \\\\\\\\\\'tools\\\\\\\\\\': empty array. Expected an array with minimum length 1, but got an empty array instead.\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\'type\\\\\\\\\\': \\\\\\\\\\'invalid_request_error\\\\\\\\\\', \\\\\\\\\\'param\\\\\\\\\\': \\\\\\\\\\'tools\\\\\\\\\\', \\\\\\\\\\'code\\\\\\\\\\': \\\\\\\\\\'empty_array\\\\\\\\\\'}}\\\\\\\\\\\\\\\\n\\\\\"\\\\\\\\n     ]\\\\\\\\n    },\\\\\\\\n    {\\\\\\\\n     \\\\\"ename\\\\\": \\\\\"AttributeError\\\\\",\\\\\\\\n     \\\\\"evalue\\\\\": \\\\\"\\\\\\\\\\'BadRequestError\\\\\\\\\\' object has no attribute \\\\\\\\\\'choices\\\\\\\\\\'\\\\\",\\\\\\\\n     \\\\\"output_type\\\\\": \\\\\"error\\\\\",\\\\\\\\n     \\\\\"traceback\\\\\": [\\\\\\\\n      \\\\\"\\\\\\\\\\\\\\\\u001b[1;31m---------------------------------------------------------------------------\\\\\\\\\\\\\\\\u001b[0m\\\\\",\\\\\\\\n      \\\\\"\\\\\\\\\\\\\\\\u001b[1;31mAttributeError\\\\\\\\\\\\\\\\u001b[0m                            Traceback (most recent call last)\\\\\",\\\\\\\\n      \\\\\"Cell \\\\\\\\\\\\\\\\u001b[1;32mIn[27], line 11\\\\\\\\\\\\\\\\u001b[0m\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m      9\\\\\\\\\\\\\\\\u001b[0m \\\\\\\\\\\\\\\\u001b[38;5;28;01mif\\\\\\\\\\\\\\\\u001b[39;00m user_input \\\\\\\\\\\\\\\\u001b[38;5;241m==\\\\\\\\\\\\\\\\u001b[39m \\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124mexit\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m:\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     10\\\\\\\\\\\\\\\\u001b[0m     \\\\\\\\\\\\\\\\u001b[38;5;28;01mbreak\\\\\\\\\\\\\\\\u001b[39;00m\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[1;32m---> 11\\\\\\\\\\\\\\\\u001b[0m response \\\\\\\\\\\\\\\\u001b[38;5;241m=\\\\\\\\\\\\\\\\u001b[39m \\\\\\\\\\\\\\\\u001b[43magent\\\\\\\\\\\\\\\\u001b[49m\\\\\\\\\\\\\\\\u001b[38;5;241;43m.\\\\\\\\\\\\\\\\u001b[39;49m\\\\\\\\\\\\\\\\u001b[43minvoke\\\\\\\\\\\\\\\\u001b[49m\\\\\\\\\\\\\\\\u001b[43m(\\\\\\\\\\\\\\\\u001b[49m\\\\\\\\\\\\\\\\u001b[43muser_input\\\\\\\\\\\\\\\\u001b[49m\\\\\\\\\\\\\\\\u001b[43m)\\\\\\\\\\\\\\\\u001b[49m\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     12\\\\\\\\\\\\\\\\u001b[0m \\\\\\\\\\\\\\\\u001b[38;5;28mprint\\\\\\\\\\\\\\\\u001b[39m(\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124mAssistant:\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m, response)\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     13\\\\\\\\\\\\\\\\u001b[0m \\\\\\\\\\\\\\\\u001b[38;5;28mprint\\\\\\\\\\\\\\\\u001b[39m()\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n      \\\\\"Cell \\\\\\\\\\\\\\\\u001b[1;32mIn[26], line 17\\\\\\\\\\\\\\\\u001b[0m, in \\\\\\\\\\\\\\\\u001b[0;36mAgent.invoke\\\\\\\\\\\\\\\\u001b[1;34m(self, message)\\\\\\\\\\\\\\\\u001b[0m\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m      8\\\\\\\\\\\\\\\\u001b[0m \\\\\\\\\\\\\\\\u001b[38;5;28mself\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mmemory\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mappend(\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m      9\\\\\\\\\\\\\\\\u001b[0m     {\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124mrole\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m: \\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124muser\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m, \\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124mcontent\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m: message}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     10\\\\\\\\\\\\\\\\u001b[0m )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     11\\\\\\\\\\\\\\\\u001b[0m chat_response \\\\\\\\\\\\\\\\u001b[38;5;241m=\\\\\\\\\\\\\\\\u001b[39m chat_completion_request(\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     12\\\\\\\\\\\\\\\\u001b[0m     messages\\\\\\\\\\\\\\\\u001b[38;5;241m=\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;28mself\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mmemory,\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     13\\\\\\\\\\\\\\\\u001b[0m     tools\\\\\\\\\\\\\\\\u001b[38;5;241m=\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;28mself\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mtools,\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     14\\\\\\\\\\\\\\\\u001b[0m     model\\\\\\\\\\\\\\\\u001b[38;5;241m=\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;28mself\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mmodel,\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     15\\\\\\\\\\\\\\\\u001b[0m )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[1;32m---> 17\\\\\\\\\\\\\\\\u001b[0m \\\\\\\\\\\\\\\\u001b[38;5;28;01mif\\\\\\\\\\\\\\\\u001b[39;00m \\\\\\\\\\\\\\\\u001b[43mchat_response\\\\\\\\\\\\\\\\u001b[49m\\\\\\\\\\\\\\\\u001b[38;5;241;43m.\\\\\\\\\\\\\\\\u001b[39;49m\\\\\\\\\\\\\\\\u001b[43mchoices\\\\\\\\\\\\\\\\u001b[49m[\\\\\\\\\\\\\\\\u001b[38;5;241m0\\\\\\\\\\\\\\\\u001b[39m]\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mfinish_reason \\\\\\\\\\\\\\\\u001b[38;5;241m==\\\\\\\\\\\\\\\\u001b[39m \\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124mstop\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m:\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     18\\\\\\\\\\\\\\\\u001b[0m     chat_response_message \\\\\\\\\\\\\\\\u001b[38;5;241m=\\\\\\\\\\\\\\\\u001b[39m chat_response\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mchoices[\\\\\\\\\\\\\\\\u001b[38;5;241m0\\\\\\\\\\\\\\\\u001b[39m]\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mmessage\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mcontent\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     19\\\\\\\\\\\\\\\\u001b[0m     \\\\\\\\\\\\\\\\u001b[38;5;28mself\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mmemory\\\\\\\\\\\\\\\\u001b[38;5;241m.\\\\\\\\\\\\\\\\u001b[39mappend(\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     20\\\\\\\\\\\\\\\\u001b[0m         {\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124mrole\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m: \\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124massistant\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m, \\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124mcontent\\\\\\\\\\\\\\\\u001b[39m\\\\\\\\\\\\\\\\u001b[38;5;124m\\\\\\\\\\'\\\\\\\\\\\\\\\\u001b[39m: chat_response_message}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\u001b[0;32m     21\\\\\\\\\\\\\\\\u001b[0m     )\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n      \\\\\"\\\\\\\\\\\\\\\\u001b[1;31mAttributeError\\\\\\\\\\\\\\\\u001b[0m: \\\\\\\\\\'BadRequestError\\\\\\\\\\' object has no attribute \\\\\\\\\\'choices\\\\\\\\\\'\\\\\"\\\\\\\\n     ]\\\\\\\\n    }\\\\\\\\n   ],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"agent = Agent()\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"agent.memory.append(\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    {\\\\\\\\\\'role\\\\\\\\\\': \\\\\\\\\\'system\\\\\\\\\\', \\\\\\\\\\'content\\\\\\\\\\': \\\\\\\\\\'You are a helpful assistant.\\\\\\\\\\'}\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\")\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"while True:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    user_input = input(\\\\\\\\\\'User Message:\\\\\\\\\\')\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    if user_input == \\\\\\\\\\'exit\\\\\\\\\\':\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        break\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    response = agent.invoke(user_input)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print(\\\\\\\\\\'Assistant:\\\\\\\\\\', response)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print()\\\\\"\\\\\\\\n   ]\\\\\\\\n  }\\\\\\\\n ],\\\\\\\\n \\\\\"metadata\\\\\": {\\\\\\\\n  \\\\\"kernelspec\\\\\": {\\\\\\\\n   \\\\\"display_name\\\\\": \\\\\"venv\\\\\",\\\\\\\\n   \\\\\"language\\\\\": \\\\\"python\\\\\",\\\\\\\\n   \\\\\"name\\\\\": \\\\\"python3\\\\\"\\\\\\\\n  },\\\\\\\\n  \\\\\"language_info\\\\\": {\\\\\\\\n   \\\\\"codemirror_mode\\\\\": {\\\\\\\\n    \\\\\"name\\\\\": \\\\\"ipython\\\\\",\\\\\\\\n    \\\\\"version\\\\\": 3\\\\\\\\n   },\\\\\\\\n   \\\\\"file_extension\\\\\": \\\\\".py\\\\\",\\\\\\\\n   \\\\\"mimetype\\\\\": \\\\\"text/x-python\\\\\",\\\\\\\\n   \\\\\"name\\\\\": \\\\\"python\\\\\",\\\\\\\\n   \\\\\"nbconvert_exporter\\\\\": \\\\\"python\\\\\",\\\\\\\\n   \\\\\"pygments_lexer\\\\\": \\\\\"ipython3\\\\\",\\\\\\\\n   \\\\\"version\\\\\": \\\\\"3.10.2\\\\\"\\\\\\\\n  }\\\\\\\\n },\\\\\\\\n \\\\\"nbformat\\\\\": 4,\\\\\\\\n \\\\\"nbformat_minor\\\\\": 2\\\\\\\\n}\\\\\\\\n\\', \\'requirements.txt\\': \\'annotated-types==0.7.0\\\\\\\\nanyio==4.4.0\\\\\\\\nasttokens==2.4.1\\\\\\\\ncertifi==2024.7.4\\\\\\\\ncharset-normalizer==3.3.2\\\\\\\\ncolorama==0.4.6\\\\\\\\ncomm==0.2.2\\\\\\\\ndebugpy==1.8.2\\\\\\\\ndecorator==5.1.1\\\\\\\\ndistro==1.9.0\\\\\\\\nexceptiongroup==1.2.1\\\\\\\\nexecuting==2.0.1\\\\\\\\nh11==0.14.0\\\\\\\\nhttpcore==1.0.5\\\\\\\\nhttpx==0.27.0\\\\\\\\nidna==3.7\\\\\\\\nipykernel==6.29.5\\\\\\\\nipython==8.26.0\\\\\\\\njedi==0.19.1\\\\\\\\njupyter_client==8.6.2\\\\\\\\njupyter_core==5.7.2\\\\\\\\nmatplotlib-inline==0.1.7\\\\\\\\nnest-asyncio==1.6.0\\\\\\\\nopenai==1.35.10\\\\\\\\npackaging==24.1\\\\\\\\nparso==0.8.4\\\\\\\\nplatformdirs==4.2.2\\\\\\\\nprompt_toolkit==3.0.47\\\\\\\\npsutil==6.0.0\\\\\\\\npure-eval==0.2.2\\\\\\\\npydantic==2.8.2\\\\\\\\npydantic_core==2.20.1\\\\\\\\nPygments==2.18.0\\\\\\\\npython-dateutil==2.9.0.post0\\\\\\\\npython-dotenv==1.0.1\\\\\\\\npywin32==306\\\\\\\\npyzmq==26.0.3\\\\\\\\nrequests==2.32.3\\\\\\\\nsix==1.16.0\\\\\\\\nsniffio==1.3.1\\\\\\\\nstack-data==0.6.3\\\\\\\\ntenacity==8.5.0\\\\\\\\ntornado==6.4.1\\\\\\\\ntqdm==4.66.4\\\\\\\\ntraitlets==5.14.3\\\\\\\\ntyping_extensions==4.12.2\\\\\\\\nurllib3==2.2.2\\\\\\\\nwcwidth==0.2.13\\\\\\\\n\\'}\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"# Example usage:\\\\n\",\\n    \"repo_data = str(fetch_github_repo(\\\\\"https://github.com/cetyz/coding-ai\\\\\"))\\\\n\",\\n    \"print(repo_data)\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.10.2\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n', 'requirements.txt': 'annotated-types==0.7.0\\nanyio==4.4.0\\nasttokens==2.4.1\\ncertifi==2024.7.4\\ncharset-normalizer==3.3.2\\ncolorama==0.4.6\\ncomm==0.2.2\\ndebugpy==1.8.2\\ndecorator==5.1.1\\ndistro==1.9.0\\nexceptiongroup==1.2.1\\nexecuting==2.0.1\\nh11==0.14.0\\nhttpcore==1.0.5\\nhttpx==0.27.0\\nidna==3.7\\nipykernel==6.29.5\\nipython==8.26.0\\njedi==0.19.1\\njupyter_client==8.6.2\\njupyter_core==5.7.2\\nmatplotlib-inline==0.1.7\\nnest-asyncio==1.6.0\\nopenai==1.35.10\\npackaging==24.1\\nparso==0.8.4\\nplatformdirs==4.2.2\\nprompt_toolkit==3.0.47\\npsutil==6.0.0\\npure-eval==0.2.2\\npydantic==2.8.2\\npydantic_core==2.20.1\\nPygments==2.18.0\\npython-dateutil==2.9.0.post0\\npython-dotenv==1.0.1\\npywin32==306\\npyzmq==26.0.3\\nrequests==2.32.3\\nsix==1.16.0\\nsniffio==1.3.1\\nstack-data==0.6.3\\ntenacity==8.5.0\\ntornado==6.4.1\\ntqdm==4.66.4\\ntraitlets==5.14.3\\ntyping_extensions==4.12.2\\nurllib3==2.2.2\\nwcwidth==0.2.13\\n'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "repo_data = str(fetch_github_repo(\"https://github.com/cetyz/coding-ai\"))\n",
    "print(repo_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
