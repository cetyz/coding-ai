{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "from assistant.tools import fetch_github_repo\n",
    "from assistant.agents import Agent\n",
    "from assistant import prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'assistant.prompts' from 'd:\\\\Documents\\\\GitHub\\\\coding-ai\\\\assistant\\\\prompts.py'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GPT_MODEL = 'gpt-3.5-turbo'\n",
    "\n",
    "client = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9hv9UvhuMOjnOM8S3OC4v80wcDYky', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None))], created=1720254824, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=GPT_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "    {'role': 'user', 'content': 'Who won the world series in 2020?'},\n",
    "    {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'},\n",
    "    {'role': 'user', 'content': 'Where was it played?'}\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print('Unable to generate ChatCompletion response')\n",
    "        print(f'Exception: {e}')\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model=GPT_MODEL):\n",
    "        self.model = model\n",
    "        self.memory = []\n",
    "        self.tools = []\n",
    "        \n",
    "    def invoke(self, message):\n",
    "        self.memory.append(\n",
    "            {'role': 'user', 'content': message}\n",
    "        )\n",
    "        chat_response = chat_completion_request(\n",
    "            messages=self.memory,\n",
    "            tools=self.tools,\n",
    "            model=self.model,\n",
    "        )\n",
    "\n",
    "        if chat_response.choices[0].finish_reason == 'stop':\n",
    "            chat_response_message = chat_response.choices[0].message.content\n",
    "            self.memory.append(\n",
    "                {'role': 'assistant', 'content': chat_response_message}\n",
    "            )\n",
    "            return chat_response_message\n",
    "\n",
    "        elif chat_response.choices[0].finish_reason == 'tool_calls':\n",
    "            # tool_calls = chat_response.choices[0].message.tool_calls\n",
    "            # for tool_call in tool_calls:\n",
    "            pass\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'github_cleaner_agent_system_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m github_cleaner_agent \u001b[38;5;241m=\u001b[39m Agent()\n\u001b[1;32m----> 3\u001b[0m github_cleaner_agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgithub_cleaner_agent_system_prompt\u001b[49m())\n\u001b[0;32m      5\u001b[0m github_cleaner_agent\u001b[38;5;241m.\u001b[39mtools\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m      6\u001b[0m     {\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     }\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     30\u001b[0m repo_data \u001b[38;5;241m=\u001b[39m fetch_github_repo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/cetyz/coding-ai\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'github_cleaner_agent_system_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "github_cleaner_agent = Agent()\n",
    "\n",
    "github_cleaner_agent.memory.append(github_cleaner_agent_system_prompt())\n",
    "\n",
    "github_cleaner_agent.tools.append(\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'placeholder_tool',\n",
    "            'description': 'Placeholder tool not to be used',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'placeholder_property': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'Placeholder property',\n",
    "                    },\n",
    "                    'placeholder_property_2': {\n",
    "                        'type': 'integer',\n",
    "                        'description': 'Placeholder property 2'\n",
    "                    },\n",
    "                },\n",
    "                'required': ['placeholder_property', 'placeholder_property_2']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "repo_data = fetch_github_repo(\"https://github.com/cetyz/coding-ai\")\n",
    "\n",
    "response = github_cleaner_agent.invoke(str(repo_data))\n",
    "print(response)\n",
    "\n",
    "# while True:\n",
    "#     user_input = input('User Message:')\n",
    "#     if user_input == 'exit':\n",
    "#         break\n",
    "#     response = agent.invoke(user_input)\n",
    "#     print('Assistant:', response)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.gitattributes': '# Auto detect text files and perform LF normalization\\n* text=auto\\n', '.gitignore': \"# Byte-compiled / optimized / DLL files\\n__pycache__/\\n*.py[cod]\\n*$py.class\\n\\n# C extensions\\n*.so\\n\\n# Distribution / packaging\\n.Python\\nbuild/\\ndevelop-eggs/\\ndist/\\ndownloads/\\neggs/\\n.eggs/\\nlib/\\nlib64/\\nparts/\\nsdist/\\nvar/\\nwheels/\\nshare/python-wheels/\\n*.egg-info/\\n.installed.cfg\\n*.egg\\nMANIFEST\\n\\n# PyInstaller\\n#  Usually these files are written by a python script from a template\\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\n*.manifest\\n*.spec\\n\\n# Installer logs\\npip-log.txt\\npip-delete-this-directory.txt\\n\\n# Unit test / coverage reports\\nhtmlcov/\\n.tox/\\n.nox/\\n.coverage\\n.coverage.*\\n.cache\\nnosetests.xml\\ncoverage.xml\\n*.cover\\n*.py,cover\\n.hypothesis/\\n.pytest_cache/\\ncover/\\n\\n# Translations\\n*.mo\\n*.pot\\n\\n# Django stuff:\\n*.log\\nlocal_settings.py\\ndb.sqlite3\\ndb.sqlite3-journal\\n\\n# Flask stuff:\\ninstance/\\n.webassets-cache\\n\\n# Scrapy stuff:\\n.scrapy\\n\\n# Sphinx documentation\\ndocs/_build/\\n\\n# PyBuilder\\n.pybuilder/\\ntarget/\\n\\n# Jupyter Notebook\\n.ipynb_checkpoints\\n\\n# IPython\\nprofile_default/\\nipython_config.py\\n\\n# pyenv\\n#   For a library or package, you might want to ignore these files since the code is\\n#   intended to run in multiple environments; otherwise, check them in:\\n# .python-version\\n\\n# pipenv\\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\\n#   install all needed dependencies.\\n#Pipfile.lock\\n\\n# poetry\\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\\n#   commonly ignored for libraries.\\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\\n#poetry.lock\\n\\n# pdm\\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\\n#pdm.lock\\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\\n#   in version control.\\n#   https://pdm.fming.dev/#use-with-ide\\n.pdm.toml\\n\\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\\n__pypackages__/\\n\\n# Celery stuff\\ncelerybeat-schedule\\ncelerybeat.pid\\n\\n# SageMath parsed files\\n*.sage.py\\n\\n# Environments\\n.env\\n.venv\\nenv/\\nvenv/\\nENV/\\nenv.bak/\\nvenv.bak/\\n\\n# Spyder project settings\\n.spyderproject\\n.spyproject\\n\\n# Rope project settings\\n.ropeproject\\n\\n# mkdocs documentation\\n/site\\n\\n# mypy\\n.mypy_cache/\\n.dmypy.json\\ndmypy.json\\n\\n# Pyre type checker\\n.pyre/\\n\\n# pytype static type analyzer\\n.pytype/\\n\\n# Cython debug symbols\\ncython_debug/\\n\\n# PyCharm\\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\\n#.idea/\\n\", 'README.md': \"# AI Coding Assistant\\n \\nAn OpenAI-powered assistant that will get actual code from your github repository.\\nThen ask it questions and it will help you.\\n\\n## Benefits\\n- No need to copy and paste snippets of your code since the assistant is able to retrieve your actual code\\n- Holistic project understanding: the assistant can better assist with overall project development since it can see your whole repo\\n\\n## Weaknesses\\n- This is still work in progress, I'll let you know\", 'playground.ipynb': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 10,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"from openai import OpenAI\\\\n\",\\n    \"from tenacity import retry, wait_random_exponential, stop_after_attempt\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 13,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"load_dotenv()\\\\n\",\\n    \"\\\\n\",\\n    \"GPT_MODEL = \\'gpt-3.5-turbo\\'\\\\n\",\\n    \"\\\\n\",\\n    \"client = OpenAI()\\\\n\",\\n    \"\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 15,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"ChatCompletion(id=\\'chatcmpl-9hv9UvhuMOjnOM8S3OC4v80wcDYky\\', choices=[Choice(finish_reason=\\'stop\\', index=0, logprobs=None, message=ChatCompletionMessage(content=\\'The 2020 World Series was played at Globe Life Field in Arlington, Texas.\\', role=\\'assistant\\', function_call=None, tool_calls=None))], created=1720254824, model=\\'gpt-3.5-turbo-0125\\', object=\\'chat.completion\\', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))\"\\n      ]\\n     },\\n     \"execution_count\": 15,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"response = client.chat.completions.create(\\\\n\",\\n    \"  model=GPT_MODEL,\\\\n\",\\n    \"  messages=[\\\\n\",\\n    \"    {\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistant.\\'},\\\\n\",\\n    \"    {\\'role\\': \\'user\\', \\'content\\': \\'Who won the world series in 2020?\\'},\\\\n\",\\n    \"    {\\'role\\': \\'assistant\\', \\'content\\': \\'The Los Angeles Dodgers won the World Series in 2020.\\'},\\\\n\",\\n    \"    {\\'role\\': \\'user\\', \\'content\\': \\'Where was it played?\\'}\\\\n\",\\n    \"  ]\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 25,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"\\'The 2020 World Series was played at Globe Life Field in Arlington, Texas.\\'\"\\n      ]\\n     },\\n     \"execution_count\": 25,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"response.choices[0].message.content\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 16,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\\\\n\",\\n    \"def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\\\\n\",\\n    \"    try:\\\\n\",\\n    \"        response = client.chat.completions.create(\\\\n\",\\n    \"            model=model,\\\\n\",\\n    \"            messages=messages,\\\\n\",\\n    \"            tools=tools,\\\\n\",\\n    \"            tool_choice=tool_choice,\\\\n\",\\n    \"        )\\\\n\",\\n    \"        return response\\\\n\",\\n    \"    except Exception as e:\\\\n\",\\n    \"        print(\\'Unable to generate ChatCompletion response\\')\\\\n\",\\n    \"        print(f\\'Exception: {e}\\')\\\\n\",\\n    \"        return e\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 26,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"class Agent:\\\\n\",\\n    \"    def __init__(self, model=GPT_MODEL):\\\\n\",\\n    \"        self.model = model\\\\n\",\\n    \"        self.memory = []\\\\n\",\\n    \"        self.tools = []\\\\n\",\\n    \"        \\\\n\",\\n    \"    def invoke(self, message):\\\\n\",\\n    \"        self.memory.append(\\\\n\",\\n    \"            {\\'role\\': \\'user\\', \\'content\\': message}\\\\n\",\\n    \"        )\\\\n\",\\n    \"        chat_response = chat_completion_request(\\\\n\",\\n    \"            messages=self.memory,\\\\n\",\\n    \"            tools=self.tools,\\\\n\",\\n    \"            model=self.model,\\\\n\",\\n    \"        )\\\\n\",\\n    \"\\\\n\",\\n    \"        if chat_response.choices[0].finish_reason == \\'stop\\':\\\\n\",\\n    \"            chat_response_message = chat_response.choices[0].message.content\\\\n\",\\n    \"            self.memory.append(\\\\n\",\\n    \"                {\\'role\\': \\'assistant\\', \\'content\\': chat_response_message}\\\\n\",\\n    \"            )\\\\n\",\\n    \"            return chat_response_message\\\\n\",\\n    \"\\\\n\",\\n    \"        elif chat_response.choices[0].finish_reason == \\'tool_calls\\':\\\\n\",\\n    \"            # to be implemented\\\\n\",\\n    \"            pass\\\\n\",\\n    \"\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 27,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Unable to generate ChatCompletion response\\\\n\",\\n      \"Exception: Error code: 400 - {\\'error\\': {\\'message\\': \\\\\"Invalid \\'tools\\': empty array. Expected an array with minimum length 1, but got an empty array instead.\\\\\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'tools\\', \\'code\\': \\'empty_array\\'}}\\\\n\"\\n     ]\\n    },\\n    {\\n     \"ename\": \"AttributeError\",\\n     \"evalue\": \"\\'BadRequestError\\' object has no attribute \\'choices\\'\",\\n     \"output_type\": \"error\",\\n     \"traceback\": [\\n      \"\\\\u001b[1;31m---------------------------------------------------------------------------\\\\u001b[0m\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m                            Traceback (most recent call last)\",\\n      \"Cell \\\\u001b[1;32mIn[27], line 11\\\\u001b[0m\\\\n\\\\u001b[0;32m      9\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m user_input \\\\u001b[38;5;241m==\\\\u001b[39m \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mexit\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m:\\\\n\\\\u001b[0;32m     10\\\\u001b[0m     \\\\u001b[38;5;28;01mbreak\\\\u001b[39;00m\\\\n\\\\u001b[1;32m---> 11\\\\u001b[0m response \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[43magent\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43minvoke\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\u001b[43muser_input\\\\u001b[49m\\\\u001b[43m)\\\\u001b[49m\\\\n\\\\u001b[0;32m     12\\\\u001b[0m \\\\u001b[38;5;28mprint\\\\u001b[39m(\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mAssistant:\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m, response)\\\\n\\\\u001b[0;32m     13\\\\u001b[0m \\\\u001b[38;5;28mprint\\\\u001b[39m()\\\\n\",\\n      \"Cell \\\\u001b[1;32mIn[26], line 17\\\\u001b[0m, in \\\\u001b[0;36mAgent.invoke\\\\u001b[1;34m(self, message)\\\\u001b[0m\\\\n\\\\u001b[0;32m      8\\\\u001b[0m \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mmemory\\\\u001b[38;5;241m.\\\\u001b[39mappend(\\\\n\\\\u001b[0;32m      9\\\\u001b[0m     {\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mrole\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m: \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124muser\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m, \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mcontent\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m: message}\\\\n\\\\u001b[0;32m     10\\\\u001b[0m )\\\\n\\\\u001b[0;32m     11\\\\u001b[0m chat_response \\\\u001b[38;5;241m=\\\\u001b[39m chat_completion_request(\\\\n\\\\u001b[0;32m     12\\\\u001b[0m     messages\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mmemory,\\\\n\\\\u001b[0;32m     13\\\\u001b[0m     tools\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mtools,\\\\n\\\\u001b[0;32m     14\\\\u001b[0m     model\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mmodel,\\\\n\\\\u001b[0;32m     15\\\\u001b[0m )\\\\n\\\\u001b[1;32m---> 17\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[43mchat_response\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mchoices\\\\u001b[49m[\\\\u001b[38;5;241m0\\\\u001b[39m]\\\\u001b[38;5;241m.\\\\u001b[39mfinish_reason \\\\u001b[38;5;241m==\\\\u001b[39m \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mstop\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m:\\\\n\\\\u001b[0;32m     18\\\\u001b[0m     chat_response_message \\\\u001b[38;5;241m=\\\\u001b[39m chat_response\\\\u001b[38;5;241m.\\\\u001b[39mchoices[\\\\u001b[38;5;241m0\\\\u001b[39m]\\\\u001b[38;5;241m.\\\\u001b[39mmessage\\\\u001b[38;5;241m.\\\\u001b[39mcontent\\\\n\\\\u001b[0;32m     19\\\\u001b[0m     \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mmemory\\\\u001b[38;5;241m.\\\\u001b[39mappend(\\\\n\\\\u001b[0;32m     20\\\\u001b[0m         {\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mrole\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m: \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124massistant\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m, \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mcontent\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m: chat_response_message}\\\\n\\\\u001b[0;32m     21\\\\u001b[0m     )\\\\n\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m: \\'BadRequestError\\' object has no attribute \\'choices\\'\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"agent = Agent()\\\\n\",\\n    \"\\\\n\",\\n    \"agent.memory.append(\\\\n\",\\n    \"    {\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistant.\\'}\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"while True:\\\\n\",\\n    \"    user_input = input(\\'User Message:\\')\\\\n\",\\n    \"    if user_input == \\'exit\\':\\\\n\",\\n    \"        break\\\\n\",\\n    \"    response = agent.invoke(user_input)\\\\n\",\\n    \"    print(\\'Assistant:\\', response)\\\\n\",\\n    \"    print()\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.10.2\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n', 'requirements.txt': 'annotated-types==0.7.0\\nanyio==4.4.0\\nasttokens==2.4.1\\ncertifi==2024.7.4\\ncharset-normalizer==3.3.2\\ncolorama==0.4.6\\ncomm==0.2.2\\ndebugpy==1.8.2\\ndecorator==5.1.1\\ndistro==1.9.0\\nexceptiongroup==1.2.1\\nexecuting==2.0.1\\nh11==0.14.0\\nhttpcore==1.0.5\\nhttpx==0.27.0\\nidna==3.7\\nipykernel==6.29.5\\nipython==8.26.0\\njedi==0.19.1\\njupyter_client==8.6.2\\njupyter_core==5.7.2\\nmatplotlib-inline==0.1.7\\nnest-asyncio==1.6.0\\nopenai==1.35.10\\npackaging==24.1\\nparso==0.8.4\\nplatformdirs==4.2.2\\nprompt_toolkit==3.0.47\\npsutil==6.0.0\\npure-eval==0.2.2\\npydantic==2.8.2\\npydantic_core==2.20.1\\nPygments==2.18.0\\npython-dateutil==2.9.0.post0\\npython-dotenv==1.0.1\\npywin32==306\\npyzmq==26.0.3\\nrequests==2.32.3\\nsix==1.16.0\\nsniffio==1.3.1\\nstack-data==0.6.3\\ntenacity==8.5.0\\ntornado==6.4.1\\ntqdm==4.66.4\\ntraitlets==5.14.3\\ntyping_extensions==4.12.2\\nurllib3==2.2.2\\nwcwidth==0.2.13\\n'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "repo_data = str(fetch_github_repo(\"https://github.com/cetyz/coding-ai\"))\n",
    "print(repo_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
